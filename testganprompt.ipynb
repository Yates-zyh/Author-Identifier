{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Generator path: C:\\Users\\HTL\\OneDrive\\Desktop\\Author-Identifier\\stylegan_output\\Charlotte_Brontë\\generator\\Charlotte_Brontë\\gan_best_model\n",
      "Discriminator path: C:\\Users\\HTL\\OneDrive\\Desktop\\Author-Identifier\\stylegan_output\\Charlotte_Brontë\\discriminator\\best_model\n",
      "Loading GPT-2 generator from C:\\Users\\HTL\\OneDrive\\Desktop\\Author-Identifier\\stylegan_output\\Charlotte_Brontë\\generator\\Charlotte_Brontë\\gan_best_model\n",
      "Successfully loaded generator model and tokenizer\n",
      "Loading BERT discriminator from C:\\Users\\HTL\\OneDrive\\Desktop\\Author-Identifier\\stylegan_output\\Charlotte_Brontë\\discriminator\\best_model\n",
      "Loaded BERT discriminator with authors: Charles_Dickens, Charlotte_Brontë, Honoré_de_Balzac, Jane_Austen, Joseph_Conrad, O._Henry, W._Somerset_Maugham, Unknown\n",
      "Generating 10 samples for Charlotte_Brontë...\n",
      "Sample 1, Reward: 0.9954\n",
      "Sample 2, Reward: 0.9963\n",
      "Sample 3, Reward: 0.9983\n",
      "Sample 4, Reward: 0.9979\n",
      "Sample 5, Reward: 0.0007\n",
      "Sample 6, Reward: 0.9964\n",
      "Sample 7, Reward: 0.9949\n",
      "Sample 8, Reward: 0.9983\n",
      "Sample 9, Reward: 0.9984\n",
      "Sample 10, Reward: 0.9984\n",
      "\n",
      "Best sample (reward=0.9984):\n",
      "A few minutes after this conversation with Mdlle. Reuter, I met her again; the most solemn and cordial of women she knew—such a personage as would never have ventured to marry me in my life! Not only did she look pale, but there was something melancholy about her face, which, not at all, I could not discern from her expression: yet it seemed to me that if any one should ever come near, or even speak ill to such a girl, they must be brought before Mrs Reed for reproof. The first visit had been made by an elderly gentleman whom we now perceived to be gone over town together. He arrived five hours ago, accompanied by Madame Beck, whose absence greatly troubled him. This time he told us what news lay on the other side of the Channel: no sooner appeared his return than Dr. John said,— \n",
      "“Mrs. Pryor is quite satisfied once more when you are out-numbered against your fortune under the circumstances\n",
      "Results saved to brontë_samples\\best_sample.txt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import logging\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, BertForSequenceClassification, BertTokenizer\n",
    "import json\n",
    "import random\n",
    "\n",
    "# 设置日志\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class GPT2Generator:\n",
    "    \"\"\"简化版的GPT-2生成器\"\"\"\n",
    "    def __init__(self, model_path):\n",
    "        print(f\"Loading GPT-2 generator from {model_path}\")\n",
    "        \n",
    "        # 直接加载本地模型和tokenizer\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_path, local_files_only=True).to(device)\n",
    "        \n",
    "        # 确保有padding token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        print(\"Successfully loaded generator model and tokenizer\")\n",
    "    \n",
    "    def generate_text(self, max_length=200, temperature=0.9, top_k=40, top_p=0.9):\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 使用模型的开始标记，让模型自己决定第一个实际单词\n",
    "        input_ids = torch.tensor([[self.tokenizer.bos_token_id]]).to(device)\n",
    "        \n",
    "        # 生成文本\n",
    "        output = self.model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=1.2,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # 解码生成的文本\n",
    "        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "\n",
    "class BERTDiscriminator:\n",
    "    \"\"\"简化版的BERT判别器\"\"\"\n",
    "    def __init__(self, model_path):\n",
    "        print(f\"Loading BERT discriminator from {model_path}\")\n",
    "        \n",
    "        # 直接加载本地模型和tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_path, local_files_only=True).to(device)\n",
    "        \n",
    "        # 加载作者标签\n",
    "        label_path = os.path.join(model_path, \"label_names.json\")\n",
    "        with open(label_path, \"r\") as f:\n",
    "            self.author_labels = json.load(f)\n",
    "        \n",
    "        # 创建作者索引映射\n",
    "        self.author_indices = {author: idx for idx, author in enumerate(self.author_labels) if author is not None}\n",
    "        print(f\"Loaded BERT discriminator with authors: {', '.join(self.author_indices.keys())}\")\n",
    "    \n",
    "    def evaluate_text(self, text):\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 编码文本\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        \n",
    "        # 预测\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # 创建作者概率字典\n",
    "        author_probs = {}\n",
    "        for author, idx in self.author_indices.items():\n",
    "            author_probs[author] = probs[0][idx].item()\n",
    "        \n",
    "        return author_probs\n",
    "\n",
    "def generate_best_sample(generator, discriminator, author, num_samples=10):\n",
    "    \"\"\"生成多个样本并选择reward最高的\"\"\"\n",
    "    samples = []\n",
    "    rewards = []\n",
    "    \n",
    "    print(f\"Generating {num_samples} samples for {author}...\")\n",
    "    for i in range(num_samples):\n",
    "        # 生成文本\n",
    "        generated_text = generator.generate_text(\n",
    "            max_length=200,\n",
    "            temperature=0.9,\n",
    "            top_k=40,\n",
    "            top_p=0.9\n",
    "        )\n",
    "        samples.append(generated_text)\n",
    "        \n",
    "        # 计算reward\n",
    "        author_probs = discriminator.evaluate_text(generated_text)\n",
    "        reward = author_probs.get(author, 0)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        print(f\"Sample {i+1}, Reward: {reward:.4f}\")\n",
    "    \n",
    "    # 找出reward最高的样本\n",
    "    best_idx = rewards.index(max(rewards))\n",
    "    best_sample = samples[best_idx]\n",
    "    best_reward = rewards[best_idx]\n",
    "    \n",
    "    print(f\"\\nBest sample (reward={best_reward:.4f}):\\n{best_sample}\")\n",
    "    \n",
    "    return best_sample, best_reward, samples, rewards\n",
    "\n",
    "# 主函数调用\n",
    "def main():\n",
    "    author = \"Charlotte_Brontë\"  # 使用正确的作者名称\n",
    "    \n",
    "    # 设置模型路径为您提供的具体路径\n",
    "    generator_path = r\"C:\\Users\\HTL\\OneDrive\\Desktop\\Author-Identifier\\stylegan_output\\Charlotte_Brontë\\generator\\Charlotte_Brontë\\gan_best_model\"\n",
    "    discriminator_path = r\"C:\\Users\\HTL\\OneDrive\\Desktop\\Author-Identifier\\stylegan_output\\Charlotte_Brontë\\discriminator\\best_model\"\n",
    "    \n",
    "    print(f\"Generator path: {generator_path}\")\n",
    "    print(f\"Discriminator path: {discriminator_path}\")\n",
    "    \n",
    "    # 加载模型\n",
    "    generator = GPT2Generator(generator_path)\n",
    "    discriminator = BERTDiscriminator(discriminator_path)\n",
    "    \n",
    "    # 生成最佳样本\n",
    "    best_sample, best_reward, all_samples, all_rewards = generate_best_sample(\n",
    "        generator, discriminator, author, num_samples=10\n",
    "    )\n",
    "    \n",
    "    # 保存结果\n",
    "    output_dir = \"brontë_samples\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    output_path = os.path.join(output_dir, \"best_sample.txt\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Best sample (reward={best_reward:.4f}):\\n\")\n",
    "        f.write(best_sample)\n",
    "        f.write(\"\\n\\n\" + \"=\"*50 + \"\\n\\n\")\n",
    "        f.write(\"All samples:\\n\\n\")\n",
    "        for i, (sample, reward) in enumerate(zip(all_samples, all_rewards)):\n",
    "            f.write(f\"Sample {i+1}, Reward: {reward:.4f}\\n\")\n",
    "            f.write(sample)\n",
    "            f.write(\"\\n\\n\" + \"-\"*50 + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"Results saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
