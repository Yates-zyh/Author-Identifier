{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BertForSequenceClassification, BertTokenizer\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import dotenv\n",
    "\n",
    "# 加载环境变量\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "class AuthorStyleAPI:\n",
    "    def __init__(self, token):\n",
    "        self.token = token\n",
    "        self.model_name = \"fjxddy/author-stylegan\"\n",
    "        self.available_authors = [\n",
    "            \"Agatha_Christie\",\n",
    "            \"Alexandre_Dumas\",\n",
    "            \"Arthur_Conan_Doyle\",\n",
    "            \"Charles_Dickens\",\n",
    "            \"Charlotte_Brontë\",\n",
    "            \"F._Scott_Fitzgerald\",\n",
    "            \"García_Márquez\",\n",
    "            \"Herman_Melville\",\n",
    "            \"Jane_Austen\",\n",
    "            \"Mark_Twain\"\n",
    "        ]\n",
    "        self.loaded_models = {} \n",
    "        self.last_request_time = None\n",
    "        self.min_request_interval = 2 \n",
    "\n",
    "    def _wait_for_rate_limit(self):\n",
    "\n",
    "        if self.last_request_time is not None:\n",
    "            elapsed = (datetime.now() - self.last_request_time).total_seconds()\n",
    "            if elapsed < self.min_request_interval:\n",
    "                time.sleep(self.min_request_interval - elapsed)\n",
    "        self.last_request_time = datetime.now()\n",
    "\n",
    "    def _load_model(self, author, max_retries=3):\n",
    "\n",
    "        if author in self.loaded_models:\n",
    "            return self.loaded_models[author]\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                self._wait_for_rate_limit()\n",
    "                print(f\"Loading model for {author}... (attempt {attempt + 1}/{max_retries})\")\n",
    "                tokenizer = AutoTokenizer.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    subfolder=f\"generators/{author}\",\n",
    "                    token=self.token\n",
    "                )\n",
    "\n",
    "                if tokenizer.pad_token is None:\n",
    "                    tokenizer.pad_token = tokenizer.eos_token\n",
    "                \n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    subfolder=f\"generators/{author}\",\n",
    "                    token=self.token\n",
    "                )\n",
    "                self.loaded_models[author] = (tokenizer, model)\n",
    "                return tokenizer, model\n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e):  \n",
    "                    wait_time = (attempt + 1) * 10  \n",
    "                    print(f\"Rate limit reached. Waiting {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Error loading model for {author}: {str(e)}\")\n",
    "                    if attempt == max_retries - 1:\n",
    "                        raise\n",
    "\n",
    "    def generate_text(self, author, num_samples=1, max_length=200, max_retries=3):\n",
    "\n",
    "        if author not in self.available_authors:\n",
    "            raise ValueError(f\"Author {author} not available. Please choose from: {', '.join(self.available_authors)}\")\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                tokenizer, model = self._load_model(author)\n",
    "                samples = []\n",
    "                \n",
    "                for i in range(num_samples):\n",
    "                    self._wait_for_rate_limit()\n",
    "                    print(f\"Generating sample {i + 1}/{num_samples}...\")\n",
    "\n",
    "                    input_ids = torch.tensor([[tokenizer.bos_token_id]])\n",
    "                    attention_mask = torch.ones_like(input_ids)\n",
    "                    \n",
    "                    outputs = model.generate(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        max_length=max_length,\n",
    "                        temperature=0.9,\n",
    "                        top_k=40,\n",
    "                        top_p=0.9,\n",
    "                        repetition_penalty=1.2,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=tokenizer.pad_token_id\n",
    "                    )\n",
    "                    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                    samples.append(generated_text)\n",
    "                \n",
    "                return samples\n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e):  \n",
    "                    wait_time = (attempt + 1) * 10  \n",
    "                    print(f\"Rate limit reached. Waiting {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Error generating text: {str(e)}\")\n",
    "                    if attempt == max_retries - 1:\n",
    "                        raise\n",
    "\n",
    "    def evaluate_text(self, text, author, max_retries=3):\n",
    "\n",
    "        if author not in self.available_authors:\n",
    "            raise ValueError(f\"Author {author} not available. Please choose from: {', '.join(self.available_authors)}\")\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                self._wait_for_rate_limit()\n",
    "\n",
    "                from transformers import BertForSequenceClassification, BertTokenizer\n",
    "                \n",
    "\n",
    "                tokenizer = BertTokenizer.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    subfolder=f\"discriminators/{author}/best_model\",\n",
    "                    token=self.token\n",
    "                )\n",
    "                model = BertForSequenceClassification.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    subfolder=f\"discriminators/{author}/best_model\",\n",
    "                    token=self.token\n",
    "                )\n",
    "                \n",
    "\n",
    "                label_path = f\"discriminators/{author}/best_model/label_names.json\"\n",
    "                try:\n",
    "                    from huggingface_hub import hf_hub_download\n",
    "                    label_file = hf_hub_download(\n",
    "                        repo_id=self.model_name,\n",
    "                        filename=label_path,\n",
    "                        token=self.token\n",
    "                    )\n",
    "                    with open(label_file, \"r\") as f:\n",
    "                        author_labels = json.load(f)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not load label names: {str(e)}\")\n",
    "                    author_labels = [None, author]  \n",
    "                \n",
    "\n",
    "                author_indices = {author: idx for idx, author in enumerate(author_labels) if author is not None}\n",
    "                author_idx = author_indices.get(author, 1)  \n",
    "                \n",
    "\n",
    "                inputs = tokenizer(\n",
    "                    text,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    max_length=512\n",
    "                )\n",
    "                \n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                    logits = outputs.logits\n",
    "                    probs = F.softmax(logits, dim=1)\n",
    "                    score = probs[0][author_idx].item()\n",
    "                \n",
    "                return score\n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e):  \n",
    "                    wait_time = (attempt + 1) * 10  \n",
    "                    print(f\"Rate limit reached. Waiting {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Error evaluating text: {str(e)}\")\n",
    "                    if attempt == max_retries - 1:\n",
    "                        raise\n",
    "\n",
    "    def generate_best_sample(self, author, num_samples=10, max_length=200):\n",
    "        \"\"\"生成多个样本并返回评分最高的一个\"\"\"\n",
    "        samples = self.generate_text(author, num_samples, max_length)\n",
    "        best_sample = None\n",
    "        best_score = -1\n",
    "        \n",
    "        for i, sample in enumerate(samples):\n",
    "            try:\n",
    "                print(f\"Evaluating sample {i + 1}/{len(samples)}...\")\n",
    "                score = self.evaluate_text(sample, author)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_sample = sample\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating sample: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return best_sample, best_score\n",
    "\n",
    "def main():\n",
    "\n",
    "    # 从环境变量获取 Hugging Face token\n",
    "    token = os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "    # 确保 token 已设置\n",
    "    if not token:\n",
    "        raise ValueError(\"请确保在 .env 文件中设置了 HUGGINGFACE_TOKEN 环境变量\")\n",
    "        \n",
    "    api = AuthorStyleAPI(token)\n",
    "    \n",
    "    print(\"Available authors:\")\n",
    "    for author in api.available_authors:\n",
    "        print(f\"- {author}\")\n",
    "    \n",
    "    while True:\n",
    "        author = input(\"\\nEnter author name (or 'quit' to exit): \")\n",
    "        if author.lower() == 'quit':\n",
    "            break\n",
    "            \n",
    "        if author not in api.available_authors:\n",
    "            print(f\"Author {author} not available. Please choose from the list above.\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            print(f\"\\nGenerating text in the style of {author}...\")\n",
    "            best_sample, score = api.generate_best_sample(author)\n",
    "            print(\"\\nBest generated text:\")\n",
    "            print(best_sample)\n",
    "            print(f\"\\nStyle match score: {score:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
