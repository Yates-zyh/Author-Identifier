{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, brown, reuters, webtext\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "发现 2 位作家: Maugham, Márquez\n",
      "作家 Maugham: 发现 2 个文本文件\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (333588 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Of_Human_Bondage.txt: 提取了 929 个样本\n",
      "  - The_Moon_ And_ Sixpence .txt: 提取了 275 个样本\n",
      "作家 Maugham: 共 1204 个样本\n",
      "作家 Márquez: 发现 3 个文本文件\n",
      "  - Big_Mama's_Funeral.txt: 提取了 19 个样本\n",
      "  - No_One_Writes_to_the_Colonel.txt: 提取了 68 个样本\n",
      "  - One_Hundred_Years_Of_Solitude.txt: 提取了 500 个样本\n",
      "作家 Márquez: 共 587 个样本\n",
      "平衡样本: 每位作家将使用 587 个样本\n",
      "  - 作家 Maugham: 从 1204 减少到 587 个样本\n",
      "目标未知作家样本数量: 1174 (占总样本数的一半)\n",
      "正在下载NLTK reuters语料库...\n",
      "NLTK reuters语料库下载完成\n",
      "从NLTK语料库收集样本...\n",
      "从Gutenberg语料库加载样本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\zyh56\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从Brown语料库加载样本...\n",
      "从Reuters语料库加载样本...\n",
      "从Webtext语料库加载样本...\n",
      "从语料库中总共收集了 20553 个样本\n",
      "已抽取 1174 个未知作家样本\n",
      "各类别样本数: [587, 587, 1174]\n",
      "总样本数: 2348\n",
      "======== Epoch 1 / 3 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:39<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.22552112632014967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  7.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证准确率: 0.9979\n",
      "验证损失: 0.0104\n",
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Maugham     0.9916    1.0000    0.9958       118\n",
      "     Márquez     1.0000    1.0000    1.0000       117\n",
      "        未知作家     1.0000    0.9957    0.9979       235\n",
      "\n",
      "    accuracy                         0.9979       470\n",
      "   macro avg     0.9972    0.9986    0.9979       470\n",
      "weighted avg     0.9979    0.9979    0.9979       470\n",
      "\n",
      "混淆矩阵:\n",
      " [[118   0   0]\n",
      " [  0 117   0]\n",
      " [  1   0 234]]\n",
      "======== Epoch 2 / 3 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:39<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.01041638583038993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证准确率: 0.9958\n",
      "验证损失: 0.0233\n",
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Maugham     0.9833    1.0000    0.9916       118\n",
      "     Márquez     1.0000    1.0000    1.0000       117\n",
      "        未知作家     1.0000    0.9915    0.9957       235\n",
      "\n",
      "    accuracy                         0.9957       470\n",
      "   macro avg     0.9944    0.9972    0.9958       470\n",
      "weighted avg     0.9958    0.9957    0.9958       470\n",
      "\n",
      "混淆矩阵:\n",
      " [[118   0   0]\n",
      " [  0 117   0]\n",
      " [  2   0 233]]\n",
      "======== Epoch 3 / 3 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:40<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.0015845272231827232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证准确率: 0.9958\n",
      "验证损失: 0.0200\n",
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Maugham     0.9833    1.0000    0.9916       118\n",
      "     Márquez     1.0000    1.0000    1.0000       117\n",
      "        未知作家     1.0000    0.9915    0.9957       235\n",
      "\n",
      "    accuracy                         0.9957       470\n",
      "   macro avg     0.9944    0.9972    0.9958       470\n",
      "weighted avg     0.9958    0.9957    0.9958       470\n",
      "\n",
      "混淆矩阵:\n",
      " [[118   0   0]\n",
      " [  0 117   0]\n",
      " [  2   0 233]]\n",
      "训练完成!\n",
      "已将作者风格模型和标签名称保存至 ../author_style_model\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,  # 若要增加作者数量，请更改此参数\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# 修改数据准备函数，支持从目录结构中读取多个作家的作品\n",
    "def prepare_data_from_directory(data_dir=\"data\", max_length=512, overlap_percent=0.3, balance_samples=True):\n",
    "    \"\"\"\n",
    "    从目录结构中准备多个作家的文本数据，并生成用于训练的数据集\n",
    "    \n",
    "    参数:\n",
    "    - data_dir: 包含作家文件夹的目录路径\n",
    "    - max_length: 文本最大token长度\n",
    "    - overlap_percent: 滑动窗口重叠比例\n",
    "    - balance_samples: 是否平衡各类别的样本数量\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    all_texts = []\n",
    "    all_labels = []\n",
    "    author_samples_dict = {}  # 用于存储每位作家的样本\n",
    "    \n",
    "    # 获取作家目录列表\n",
    "    author_dirs = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    print(f\"发现 {len(author_dirs)} 位作家: {', '.join(author_dirs)}\")\n",
    "    \n",
    "    label_names = author_dirs + [\"未知作家\"]\n",
    "    \n",
    "    # 读取每位作家的文本\n",
    "    for idx, author_name in enumerate(author_dirs):\n",
    "        author_path = os.path.join(data_dir, author_name)\n",
    "        author_texts = []\n",
    "        \n",
    "        # 获取该作家的所有txt文件\n",
    "        txt_files = [f for f in os.listdir(author_path) if f.endswith('.txt')]\n",
    "        print(f\"作家 {author_name}: 发现 {len(txt_files)} 个文本文件\")\n",
    "        \n",
    "        # 读取每个文件并添加到作家文本列表\n",
    "        for txt_file in txt_files:\n",
    "            file_path = os.path.join(author_path, txt_file)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    file_text = f.read()\n",
    "                    \n",
    "                # 对每个文件内容创建样本\n",
    "                file_samples = create_sliding_window_samples(\n",
    "                    file_text, \n",
    "                    tokenizer, \n",
    "                    max_length=max_length, \n",
    "                    overlap_tokens=int(max_length * overlap_percent)\n",
    "                )\n",
    "                author_texts.extend(file_samples)\n",
    "                print(f\"  - {txt_file}: 提取了 {len(file_samples)} 个样本\")\n",
    "            except Exception as e:\n",
    "                print(f\"  - 无法读取 {txt_file}: {str(e)}\")\n",
    "        \n",
    "        # 存储该作家的所有样本\n",
    "        author_samples_dict[author_name] = author_texts\n",
    "        print(f\"作家 {author_name}: 共 {len(author_texts)} 个样本\")\n",
    "    \n",
    "    # 如果需要平衡样本，找出最小样本数量\n",
    "    if balance_samples:\n",
    "        min_author_samples = min([len(samples) for samples in author_samples_dict.values()])\n",
    "        print(f\"平衡样本: 每位作家将使用 {min_author_samples} 个样本\")\n",
    "        \n",
    "        # 限制每位作家的样本数量\n",
    "        for author_name, samples in author_samples_dict.items():\n",
    "            if len(samples) > min_author_samples:\n",
    "                # 随机选择样本子集\n",
    "                author_samples_dict[author_name] = random.sample(samples, min_author_samples)\n",
    "                print(f\"  - 作家 {author_name}: 从 {len(samples)} 减少到 {min_author_samples} 个样本\")\n",
    "    \n",
    "    # 将各作家样本添加到数据集\n",
    "    for idx, (author_name, samples) in enumerate(author_samples_dict.items()):\n",
    "        all_texts.extend(samples)\n",
    "        all_labels.extend([idx] * len(samples))\n",
    "    \n",
    "    # 计算需要的\"未知作家\"样本数量（等于所有已知作家样本总和，占总样本数的一半）\n",
    "    target_unknown_samples = len(author_samples_dict) * min_author_samples\n",
    "    print(f\"目标未知作家样本数量: {target_unknown_samples} (占总样本数的一半)\")\n",
    "    \n",
    "    # 生成\"未知作家\"样本 - 使用多个NLTK语料库\n",
    "    unknown_samples = []\n",
    "    \n",
    "    # 确保NLTK语料库已下载\n",
    "    corpus_list = ['gutenberg', 'brown', 'reuters', 'webtext']\n",
    "    for corpus_name in corpus_list:\n",
    "        try:\n",
    "            nltk.data.find(f'corpora/{corpus_name}')\n",
    "        except LookupError:\n",
    "            print(f\"正在下载NLTK {corpus_name}语料库...\")\n",
    "            nltk.download(corpus_name)\n",
    "            print(f\"NLTK {corpus_name}语料库下载完成\")\n",
    "    \n",
    "    # 从所有语料库收集样本\n",
    "    all_corpus_samples = []\n",
    "    print(\"从NLTK语料库收集样本...\")\n",
    "    \n",
    "    # 1. Gutenberg语料库\n",
    "    print(\"从Gutenberg语料库加载样本...\")\n",
    "    for fileid in gutenberg.fileids():\n",
    "        text = gutenberg.raw(fileid)\n",
    "        samples = create_sliding_window_samples(\n",
    "            text, \n",
    "            tokenizer, \n",
    "            max_length=max_length, \n",
    "            overlap_tokens=int(max_length * overlap_percent),\n",
    "            max_samples=50  # 限制每个文件的样本数\n",
    "        )\n",
    "        all_corpus_samples.extend(samples)\n",
    "    \n",
    "    # 2. Brown语料库\n",
    "    print(\"从Brown语料库加载样本...\")\n",
    "    for fileid in brown.fileids():\n",
    "        text = brown.raw(fileid)\n",
    "        samples = create_sliding_window_samples(\n",
    "            text, \n",
    "            tokenizer, \n",
    "            max_length=max_length, \n",
    "            overlap_tokens=int(max_length * overlap_percent),\n",
    "            max_samples=25  # 限制每个文件的样本数\n",
    "        )\n",
    "        all_corpus_samples.extend(samples)\n",
    "    \n",
    "    # 3. Reuters语料库\n",
    "    print(\"从Reuters语料库加载样本...\")\n",
    "    for fileid in reuters.fileids():\n",
    "        text = reuters.raw(fileid)\n",
    "        samples = create_sliding_window_samples(\n",
    "            text, \n",
    "            tokenizer, \n",
    "            max_length=max_length, \n",
    "            overlap_tokens=int(max_length * overlap_percent),\n",
    "            max_samples=15  # 限制每个文件的样本数\n",
    "        )\n",
    "        all_corpus_samples.extend(samples)\n",
    "    \n",
    "    # 4. Webtext语料库\n",
    "    print(\"从Webtext语料库加载样本...\")\n",
    "    for fileid in webtext.fileids():\n",
    "        text = webtext.raw(fileid)\n",
    "        samples = create_sliding_window_samples(\n",
    "            text, \n",
    "            tokenizer, \n",
    "            max_length=max_length, \n",
    "            overlap_tokens=int(max_length * overlap_percent),\n",
    "            max_samples=10  # 限制每个文件的样本数\n",
    "        )\n",
    "        all_corpus_samples.extend(samples)\n",
    "    \n",
    "    print(f\"从语料库中总共收集了 {len(all_corpus_samples)} 个样本\")\n",
    "    \n",
    "    # 随机抽样以获取目标数量的未知作家样本\n",
    "    if len(all_corpus_samples) < target_unknown_samples:\n",
    "        print(f\"警告：语料库样本不足，只能使用 {len(all_corpus_samples)} 个样本\")\n",
    "        unknown_samples = all_corpus_samples\n",
    "    else:\n",
    "        unknown_samples = random.sample(all_corpus_samples, target_unknown_samples)\n",
    "    print(f\"已抽取 {len(unknown_samples)} 个未知作家样本\")\n",
    "    \n",
    "    # 将\"未知作家\"样本添加到数据集\n",
    "    unknown_label = len(author_dirs)  # 最后一个标签是\"未知作家\"\n",
    "    all_texts.extend(unknown_samples)\n",
    "    all_labels.extend([unknown_label] * len(unknown_samples))\n",
    "    \n",
    "    # 打印数据集统计信息\n",
    "    label_counts = [all_labels.count(i) for i in range(len(label_names))]\n",
    "    print(f\"各类别样本数: {label_counts}\")\n",
    "    print(f\"总样本数: {len(all_texts)}\")\n",
    "    \n",
    "    # 对文本进行分词和编码\n",
    "    encoded_data = tokenizer(\n",
    "        all_texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # 创建数据集\n",
    "    input_ids = encoded_data['input_ids']\n",
    "    attention_masks = encoded_data['attention_mask']\n",
    "    labels = torch.tensor(all_labels)\n",
    "    \n",
    "    # 分割为训练集和验证集\n",
    "    train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "        input_ids, attention_masks, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    # 创建DataLoaders\n",
    "    batch_size = 8\n",
    "    \n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    \n",
    "    val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, label_names\n",
    "\n",
    "# 添加一个新函数，用于创建滑动窗口样本\n",
    "def create_sliding_window_samples(text, tokenizer, max_length, overlap_tokens, max_samples=None):\n",
    "    \"\"\"\n",
    "    使用滑动窗口方法从长文本中创建样本\n",
    "    \n",
    "    参数:\n",
    "    - text: 输入文本\n",
    "    - tokenizer: 分词器\n",
    "    - max_length: 窗口的最大token长度\n",
    "    - overlap_tokens: 相邻窗口之间重叠的token数量\n",
    "    - max_samples: 最大样本数量限制\n",
    "    \n",
    "    返回:\n",
    "    - samples: 文本样本列表\n",
    "    \"\"\"\n",
    "    # 清理文本，移除多余空白符\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # 对整个文本进行分词，获取tokens\n",
    "    tokens = tokenizer.encode(text)\n",
    "    \n",
    "    samples = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    # 使用滑动窗口切分文本\n",
    "    while start_idx < len(tokens):\n",
    "        # 确保不超出文本长度\n",
    "        end_idx = min(start_idx + max_length, len(tokens))\n",
    "        \n",
    "        # 提取当前窗口的tokens\n",
    "        window_tokens = tokens[start_idx:end_idx]\n",
    "        \n",
    "        # 仅保留足够长的窗口（至少100个token）\n",
    "        if len(window_tokens) >= 100:\n",
    "            # 将tokens转回文本\n",
    "            window_text = tokenizer.decode(window_tokens)\n",
    "            samples.append(window_text)\n",
    "        \n",
    "        # 如果已经到达文本末尾，退出循环\n",
    "        if end_idx == len(tokens):\n",
    "            break\n",
    "            \n",
    "        # 更新下一个窗口的起始位置（考虑重叠）\n",
    "        start_idx += (max_length - overlap_tokens)\n",
    "        \n",
    "        # 如果达到样本数量限制，提前结束\n",
    "        if max_samples and len(samples) >= max_samples:\n",
    "            break\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(train_dataloader, val_dataloader, label_names, epochs):\n",
    "    # 添加权重衰减和学习率调整\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8, weight_decay=0.01)\n",
    "    \n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    \n",
    "    # Create the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=int(total_steps * 0.1),  # 10% 的步骤用于热身\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # 用于跟踪指标\n",
    "    best_val_accuracy = 0\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f'======== Epoch {epoch + 1} / {epochs} ========')\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_dataloader):\n",
    "            model.zero_grad()\n",
    "            \n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(f\"Average training loss: {avg_train_loss}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for batch in tqdm(val_dataloader):\n",
    "            with torch.no_grad():\n",
    "                input_ids = batch[0].to(device)\n",
    "                attention_mask = batch[1].to(device)\n",
    "                labels = batch[2].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                total_eval_loss += loss.item()\n",
    "                \n",
    "                logits = outputs.logits\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                # 收集预测和标签用于计算F1分数\n",
    "                all_preds.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                accuracy = (predictions == labels).float().mean().item()\n",
    "                total_eval_accuracy += accuracy\n",
    "        \n",
    "        avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
    "        avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "        \n",
    "        # 计算F1分数和其他指标\n",
    "        from sklearn.metrics import classification_report, confusion_matrix\n",
    "        report = classification_report(all_labels, all_preds, target_names=label_names, digits=4)\n",
    "        conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "        \n",
    "        print(f\"验证准确率: {avg_val_accuracy:.4f}\")\n",
    "        print(f\"验证损失: {avg_val_loss:.4f}\")\n",
    "        print(\"分类报告:\\n\", report)\n",
    "        print(\"混淆矩阵:\\n\", conf_matrix)\n",
    "    \n",
    "    print(\"训练完成!\")\n",
    "    return model\n",
    "\n",
    "# 运行训练\n",
    "# 使用新的数据准备函数\n",
    "train_dataloader, val_dataloader, label_names = prepare_data_from_directory(data_dir=\"data\", balance_samples=True)\n",
    "\n",
    "# 更新训练函数调用\n",
    "fine_tuned_model = train_model(train_dataloader, val_dataloader, label_names, epochs=3)\n",
    "\n",
    "# 保存模型和标签名称\n",
    "model_save_path = \"../author_style_model\"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# 同时保存标签名称到JSON文件\n",
    "import json\n",
    "with open(f\"{model_save_path}/label_names.json\", 'w') as f:\n",
    "    json.dump(label_names, f)\n",
    "    \n",
    "print(f\"已将作者风格模型和标签名称保存至 {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本: A police inspector had come forward with a very yo...\n",
      "预测作家: Márquez\n",
      "置信度: 0.95\n",
      "所有类别概率:\n",
      "  - Maugham: 0.0444\n",
      "  - Márquez: 0.9549\n",
      "  - 未知作家: 0.0007\n",
      "--------------------------------------------------\n",
      "文本: I confess that when first I made acquaintance with...\n",
      "预测作家: Maugham\n",
      "置信度: 1.00\n",
      "所有类别概率:\n",
      "  - Maugham: 0.9992\n",
      "  - Márquez: 0.0003\n",
      "  - 未知作家: 0.0005\n",
      "--------------------------------------------------\n",
      "文本: Our team has decided to create the dataset ourselv...\n",
      "预测作家: 未知作家\n",
      "置信度: 1.00\n",
      "所有类别概率:\n",
      "  - Maugham: 0.0013\n",
      "  - Márquez: 0.0008\n",
      "  - 未知作家: 0.9979\n",
      "--------------------------------------------------\n",
      "文本: In recent years, the rapid development of large la...\n",
      "预测作家: 未知作家\n",
      "置信度: 1.00\n",
      "所有类别概率:\n",
      "  - Maugham: 0.0017\n",
      "  - Márquez: 0.0016\n",
      "  - 未知作家: 0.9966\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# 加载保存的模型和分词器\n",
    "model_path = \"../author_style_model\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 预测部分\n",
    "def analyze_text_style(text, confidence_threshold=0.6):\n",
    "    \"\"\"分析文本属于哪位作家的写作风格，或判断为未知作家\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "    \n",
    "    # 获取最高概率及其对应的类别\n",
    "    max_prob, predicted_class = torch.max(probabilities, dim=1)\n",
    "    max_prob = max_prob.item()\n",
    "    predicted_class = predicted_class.item()\n",
    "    \n",
    "    # 载入标签名称\n",
    "    with open(f\"{model_path}/label_names.json\", 'r') as f:\n",
    "        label_names = json.load(f)\n",
    "    \n",
    "    # 如果最高概率低于阈值，认为是未知作家\n",
    "    if max_prob < confidence_threshold:\n",
    "        result = {\n",
    "            \"predicted_author\": \"未知作家\",\n",
    "            \"confidence\": 1 - max_prob,  # 未知的置信度\n",
    "            \"probabilities\": {name: prob.item() for name, prob in zip(label_names, probabilities[0])}\n",
    "        }\n",
    "    else:\n",
    "        result = {\n",
    "            \"predicted_author\": label_names[predicted_class],\n",
    "            \"confidence\": max_prob,\n",
    "            \"probabilities\": {name: prob.item() for name, prob in zip(label_names, probabilities[0])}\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 测试示例\n",
    "test_texts = [\n",
    "    # Márquez\n",
    "    \"A police inspector had come forward with a very young medical student who was completing his forensic training at the municipal dispensary, and it was they who had ventilated the room and covered the body while waiting for Dr. Urbino to arrive. They greeted him with a solemnity that on this occasion had more of condolence than veneration, for no one was unaware of the degree of his friendship with Jeremiah de Saint-Amour. The eminent teacher shook hands with each of them, as he always did with every one of his pupils before beginning the daily class in general clinical medicine, and then, as if it were a flower, he grasped the hem of the blanket with the tips of his index finger and his thumb, and slowly uncovered the body with sacramental circumspection. Jeremiah de Saint-Amour was completely naked, stiff and twisted, eyes open, body blue, looking fifty years older than he had the night before. He had luminous pupils, yellowish beard and hair, and an old scar sewn with baling knots across his stomach. The use of crutches had made his torso and arms as broad as a galley slave’s, but his defenseless legs looked like an orphan’s. Dr. Juvenal Urbino studied him for a moment, his heart aching as it rarely had in the long years of his futile struggle against death.\",\n",
    "\n",
    "    # Maugham\n",
    "    \"I confess that when first I made acquaintance with Charles Strickland I never for a moment discerned that there was in him anything out of the ordinary. Yet now few will be found to deny his greatness. I do not speak of that greatness which is achieved by the fortunate politician or the successful soldier; that is a quality which belongs to the place he occupies rather than to the man; and a change of circumstances reduces it to very discreet proportions. The Prime Minister out of office is seen, too often, to have been but a pompous rhetorician, and the General without at! army is but the tame hero of a market town. The greatness of Charles Strickland was authentic. It may be that you do not like his art, but at all events you can hardly refuse it the tribute of your interest. He disturbs and arrests. The time has passed when he was an object of ridicule, and it is no longer a mark of eccentricity to defend or of perversity to extol him. His faults are accepted as the necessary complement to his merits. It is still possible to discuss his place in art, and the adulation of his admirers is perhaps no less capricious than the .disparagement of his detractors; but one thing cantilever be doubtful, and that is that he had genius. Tjo rqfy mind the most interesting thing in art is the personality of the artist ; and if that is singu- lar, I am willing to excuse a thousand faults. I suppose Velasquez was a better painter than El Greco, but custom stales one’s admiration for him: the Cretan, sensual and tragic, proffers the mystery of his soul like a standing sacrifice. The artist, painter, poet, or musician, by his decoration, sublime or beautiful, satisfies the aesthetic sense; but that is akin to the sexual instinct, anjl shares its barbarity: he lays before you also the greater gift of himself. To pursue his secret has something of the fascination of a detective story. It is a riddle which shares with the universe the merit of having no answer. The most insignificant of Strickland’s works suggests a personality which is strange, tormented, and complex; and it is this surely which prevents even those who do not like his pictures from being indifferent to them; it is this which has excited so curious an interest in his life and character. \",\n",
    "    \n",
    "    # Unknown author\n",
    "    \"Our team has decided to create the dataset ourselves, consisting of articles from famous authors. The dataset structure includes work title, author, article content, and sentiment labels (positive, neutral, negative). We will select both Chinese and English articles in their original forms to avoid translation biases. Preprocessing will standardize the text by removing punctuation, converting text to lowercase, removing stopwords, and applying other necessary preprocessing techniques. Sentiment labels will be added through manual annotation and leveraging publicly available sentiment datasets. Our primary approach is to fine-tune BERT based on the collected dataset. Given our multilingual dataset, we have opted to use Multilingual BERT. If initial results are unsatisfactory, separate models may be trained for Chinese and English texts. Considering BERT's token limit (512 tokens), we plan to employ a sliding window segmentation approach. Should segmentation negatively impact stylistic judgment, we will explore alternative models like LSTM combined with BERT to maintain context continuity. Additionally, we will fine-tune BERT or RoBERTa for sentiment analysis, potentially incorporating multi-task learning to simultaneously optimize authorship and sentiment classification tasks.\",\n",
    "\n",
    "    # Unknown author\n",
    "    \"In recent years, the rapid development of large language models (LLMs) has reshaped artificial intelligence research and applications. DeepSeek is an open-source AI research initiative dedicated to advancing natural language processing (NLP) through state-of-the-art LLMs. By offering open-access models and tools, DeepSeek aims to democratize AI capabilities and accelerate innovation in various fields. DeepSeek was established to address the growing need for transparency and accessibility in AI research. Unlike proprietary models from major tech companies, DeepSeek provides publicly available models that can be fine-tuned and deployed by researchers, developers, and businesses. This open approach fosters collaboration and ensures that AI advancements benefit a wider audience. DeepSeek's models are designed to support a wide range of NLP tasks, including text classification, sentiment analysis, question answering, and more. By leveraging large-scale pre-trained models, DeepSeek enables users to achieve high performance on various NLP benchmarks with minimal effort. DeepSeek's models are built on top of the Hugging Face Transformers library, a popular open-source framework for training and deploying transformer models. This integration allows users to easily access and utilize DeepSeek's models within their existing workflows. DeepSeek's models are available in multiple languages, making them suitable for global applications. Whether you're working on English, Chinese, Spanish, or other languages, DeepSeek provides models that can be fine-tuned for specific tasks and domains. DeepSeek's models are trained on diverse datasets to ensure robust performance across different languages and domains. By incorporating multilingual data and transfer learning techniques, DeepSeek's models can effectively handle various NLP tasks with high accuracy and efficiency. DeepSeek's models are continuously updated and refined to reflect the latest advancements in NLP research. By staying at the forefront of AI innovation, DeepSeek aims to empower users with cutting-edge tools and technologies for natural language processing. DeepSeek's mission is to democratize AI research and foster collaboration among researchers, developers, and businesses. By providing open-access models and resources, DeepSeek enables users to leverage state-of-the-art AI capabilities for a wide range of applications. Whether you're a student, researcher, developer, or business professional, DeepSeek offers tools and models that can accelerate your AI projects and drive innovation in the field of natural language processing.\",\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = analyze_text_style(text)\n",
    "    print(f\"文本: {text[:50]}...\")\n",
    "    print(f\"预测作家: {result['predicted_author']}\")\n",
    "    print(f\"置信度: {result['confidence']:.2f}\")\n",
    "    print(\"所有类别概率:\")\n",
    "    for author, prob in result['probabilities'].items():\n",
    "        print(f\"  - {author}: {prob:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
