{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总样本数: 2198, 正样本: 848, 负样本: 1350\n",
      "======== Epoch 1 / 10 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:47<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.36558865130646157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:03<00:00, 16.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证准确率: 0.9750\n",
      "验证损失: 0.0801\n",
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       非目标作者     0.9814    0.9778    0.9796       270\n",
      "        目标作者     0.9649    0.9706    0.9677       170\n",
      "\n",
      "    accuracy                         0.9750       440\n",
      "   macro avg     0.9732    0.9742    0.9737       440\n",
      "weighted avg     0.9750    0.9750    0.9750       440\n",
      "\n",
      "混淆矩阵:\n",
      " [[264   6]\n",
      " [  5 165]]\n",
      "======== Epoch 2 / 10 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:46<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.08574208684916482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:03<00:00, 17.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证准确率: 0.9795\n",
      "验证损失: 0.0723\n",
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       非目标作者     0.9962    0.9704    0.9831       270\n",
      "        目标作者     0.9548    0.9941    0.9741       170\n",
      "\n",
      "    accuracy                         0.9795       440\n",
      "   macro avg     0.9755    0.9822    0.9786       440\n",
      "weighted avg     0.9802    0.9795    0.9796       440\n",
      "\n",
      "混淆矩阵:\n",
      " [[262   8]\n",
      " [  1 169]]\n",
      "======== Epoch 3 / 10 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:46<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.017384177094010574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:03<00:00, 16.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证准确率: 0.9773\n",
      "验证损失: 0.0802\n",
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       非目标作者     0.9815    0.9815    0.9815       270\n",
      "        目标作者     0.9706    0.9706    0.9706       170\n",
      "\n",
      "    accuracy                         0.9773       440\n",
      "   macro avg     0.9760    0.9760    0.9760       440\n",
      "weighted avg     0.9773    0.9773    0.9773       440\n",
      "\n",
      "混淆矩阵:\n",
      " [[265   5]\n",
      " [  5 165]]\n",
      "======== Epoch 4 / 10 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:46<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.011827457018014022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:03<00:00, 16.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证准确率: 0.9636\n",
      "验证损失: 0.2126\n",
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       非目标作者     1.0000    0.9407    0.9695       270\n",
      "        目标作者     0.9140    1.0000    0.9551       170\n",
      "\n",
      "    accuracy                         0.9636       440\n",
      "   macro avg     0.9570    0.9704    0.9623       440\n",
      "weighted avg     0.9668    0.9636    0.9639       440\n",
      "\n",
      "混淆矩阵:\n",
      " [[254  16]\n",
      " [  0 170]]\n",
      "======== Epoch 5 / 10 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:46<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.0012716856258273104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:03<00:00, 17.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证准确率: 0.9841\n",
      "验证损失: 0.1155\n",
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       非目标作者     1.0000    0.9741    0.9869       270\n",
      "        目标作者     0.9605    1.0000    0.9798       170\n",
      "\n",
      "    accuracy                         0.9841       440\n",
      "   macro avg     0.9802    0.9870    0.9833       440\n",
      "weighted avg     0.9847    0.9841    0.9841       440\n",
      "\n",
      "混淆矩阵:\n",
      " [[263   7]\n",
      " [  0 170]]\n",
      "======== Epoch 6 / 10 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:46<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 8.410241225647042e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:03<00:00, 17.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证准确率: 0.9818\n",
      "验证损失: 0.1074\n",
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       非目标作者     0.9962    0.9741    0.9850       270\n",
      "        目标作者     0.9602    0.9941    0.9769       170\n",
      "\n",
      "    accuracy                         0.9818       440\n",
      "   macro avg     0.9782    0.9841    0.9809       440\n",
      "weighted avg     0.9823    0.9818    0.9819       440\n",
      "\n",
      "混淆矩阵:\n",
      " [[263   7]\n",
      " [  1 169]]\n",
      "======== Epoch 7 / 10 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:46<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 6.629286776429085e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:03<00:00, 17.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证准确率: 0.9818\n",
      "验证损失: 0.1086\n",
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       非目标作者     0.9962    0.9741    0.9850       270\n",
      "        目标作者     0.9602    0.9941    0.9769       170\n",
      "\n",
      "    accuracy                         0.9818       440\n",
      "   macro avg     0.9782    0.9841    0.9809       440\n",
      "weighted avg     0.9823    0.9818    0.9819       440\n",
      "\n",
      "混淆矩阵:\n",
      " [[263   7]\n",
      " [  1 169]]\n",
      "======== Epoch 8 / 10 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:46<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 5.570992699094032e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:03<00:00, 17.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证准确率: 0.9818\n",
      "验证损失: 0.1106\n",
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       非目标作者     0.9962    0.9741    0.9850       270\n",
      "        目标作者     0.9602    0.9941    0.9769       170\n",
      "\n",
      "    accuracy                         0.9818       440\n",
      "   macro avg     0.9782    0.9841    0.9809       440\n",
      "weighted avg     0.9823    0.9818    0.9819       440\n",
      "\n",
      "混淆矩阵:\n",
      " [[263   7]\n",
      " [  1 169]]\n",
      "======== Epoch 9 / 10 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:46<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 5.1212503636964934e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:03<00:00, 17.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证准确率: 0.9818\n",
      "验证损失: 0.1118\n",
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       非目标作者     0.9962    0.9741    0.9850       270\n",
      "        目标作者     0.9602    0.9941    0.9769       170\n",
      "\n",
      "    accuracy                         0.9818       440\n",
      "   macro avg     0.9782    0.9841    0.9809       440\n",
      "weighted avg     0.9823    0.9818    0.9819       440\n",
      "\n",
      "混淆矩阵:\n",
      " [[263   7]\n",
      " [  1 169]]\n",
      "======== Epoch 10 / 10 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:46<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 4.982007503713248e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:03<00:00, 17.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证准确率: 0.9818\n",
      "验证损失: 0.1124\n",
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       非目标作者     0.9962    0.9741    0.9850       270\n",
      "        目标作者     0.9602    0.9941    0.9769       170\n",
      "\n",
      "    accuracy                         0.9818       440\n",
      "   macro avg     0.9782    0.9841    0.9809       440\n",
      "weighted avg     0.9823    0.9818    0.9819       440\n",
      "\n",
      "混淆矩阵:\n",
      " [[263   7]\n",
      " [  1 169]]\n",
      "训练完成!\n",
      "已将作者风格模型保存至 ../author_style_model\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"  # You can change to other BERT variants\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  # 我们将使用二分类：1表示目标作者的风格，0表示非目标作者\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# 加载和准备作者文本数据\n",
    "def prepare_data(author_file_path, other_authors_file_path=None, max_length=256):\n",
    "    \"\"\"\n",
    "    加载作者的文本数据，并准备用于训练的数据集\n",
    "    \n",
    "    参数:\n",
    "    - author_file_path: 目标作者作品的文件路径\n",
    "    - other_authors_file_path: 其他作者作品的文件路径(用于对比)\n",
    "    - max_length: 文本最大长度\n",
    "    \"\"\"\n",
    "    # 读取目标作者的文本\n",
    "    with open(author_file_path, 'r', encoding='utf-8') as f:\n",
    "        author_text = f.read()\n",
    "    \n",
    "    # 将作者文本分割成段落或句子\n",
    "    author_samples = []\n",
    "    # 可以按段落分割\n",
    "    paragraphs = author_text.split('\\n\\n')\n",
    "    for para in paragraphs:\n",
    "        if len(para.strip()) > 50:  # 只保留有意义的段落\n",
    "            author_samples.append(para.strip())\n",
    "    \n",
    "    # 准备标签(1表示目标作者)\n",
    "    author_labels = [1] * len(author_samples)\n",
    "    \n",
    "    # 如果提供了对比文本，也加载它们\n",
    "    other_samples = []\n",
    "    other_labels = []\n",
    "    if other_authors_file_path:\n",
    "        with open(other_authors_file_path, 'r', encoding='utf-8') as f:\n",
    "            other_text = f.read()\n",
    "        paragraphs = other_text.split('\\n\\n')\n",
    "        for para in paragraphs:\n",
    "            if len(para.strip()) > 50:\n",
    "                other_samples.append(para.strip())\n",
    "        other_labels = [0] * len(other_samples)\n",
    "    \n",
    "    # 合并所有样本和标签\n",
    "    all_texts = author_samples + other_samples\n",
    "    all_labels = author_labels + other_labels\n",
    "    \n",
    "    # 确保数据集平衡\n",
    "    pos_samples = [i for i, label in enumerate(all_labels) if label == 1]\n",
    "    neg_samples = [i for i, label in enumerate(all_labels) if label == 0]\n",
    "    \n",
    "    # 如果正样本太多，减少一些\n",
    "    if len(pos_samples) > len(neg_samples) * 2:\n",
    "        remove_indices = random.sample(pos_samples, len(pos_samples) - len(neg_samples) * 2)\n",
    "        all_texts = [text for i, text in enumerate(all_texts) if i not in remove_indices]\n",
    "        all_labels = [label for i, label in enumerate(all_labels) if i not in remove_indices]\n",
    "    \n",
    "    print(f\"总样本数: {len(all_texts)}, 正样本: {sum(all_labels)}, 负样本: {len(all_labels) - sum(all_labels)}\")\n",
    "    \n",
    "    # 对文本进行分词和编码\n",
    "    encoded_data = tokenizer(\n",
    "        all_texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # 创建数据集\n",
    "    input_ids = encoded_data['input_ids']\n",
    "    attention_masks = encoded_data['attention_mask']\n",
    "    labels = torch.tensor(all_labels)\n",
    "    \n",
    "    # 分割为训练集和验证集\n",
    "    train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "        input_ids, attention_masks, labels, test_size=0.2, random_state=seed_val, stratify=labels\n",
    "    )\n",
    "    \n",
    "    # 创建DataLoaders\n",
    "    batch_size = 8\n",
    "    \n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    \n",
    "    val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "    \n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(train_dataloader, val_dataloader, epochs=4):\n",
    "    # 添加权重衰减和学习率调整\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8, weight_decay=0.01)\n",
    "    \n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    \n",
    "    # Create the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=int(total_steps * 0.1),  # 10% 的步骤用于热身\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # 用于跟踪指标\n",
    "    best_val_accuracy = 0\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f'======== Epoch {epoch + 1} / {epochs} ========')\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_dataloader):\n",
    "            model.zero_grad()\n",
    "            \n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(f\"Average training loss: {avg_train_loss}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for batch in tqdm(val_dataloader):\n",
    "            with torch.no_grad():\n",
    "                input_ids = batch[0].to(device)\n",
    "                attention_mask = batch[1].to(device)\n",
    "                labels = batch[2].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                total_eval_loss += loss.item()\n",
    "                \n",
    "                logits = outputs.logits\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                # 收集预测和标签用于计算F1分数\n",
    "                all_preds.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                accuracy = (predictions == labels).float().mean().item()\n",
    "                total_eval_accuracy += accuracy\n",
    "        \n",
    "        avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
    "        avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "        \n",
    "        # 计算F1分数和其他指标\n",
    "        from sklearn.metrics import classification_report, confusion_matrix\n",
    "        report = classification_report(all_labels, all_preds, target_names=['非目标作者', '目标作者'], digits=4)\n",
    "        conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "        \n",
    "        print(f\"验证准确率: {avg_val_accuracy:.4f}\")\n",
    "        print(f\"验证损失: {avg_val_loss:.4f}\")\n",
    "        print(\"分类报告:\\n\", report)\n",
    "        print(\"混淆矩阵:\\n\", conf_matrix)\n",
    "    \n",
    "    print(\"训练完成!\")\n",
    "    return model\n",
    "\n",
    "# 运行训练\n",
    "author_file_path = \"author_works.txt\"\n",
    "other_authors_file_path = \"author_works2.txt\"\n",
    "\n",
    "# 尝试使用其他作者文本\n",
    "train_dataloader, val_dataloader = prepare_data(author_file_path, other_authors_file_path)\n",
    "fine_tuned_model = train_model(train_dataloader, val_dataloader, epochs=10)\n",
    "\n",
    "# 保存模型\n",
    "model_save_path = \"../author_style_model\"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"已将作者风格模型保存至 {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本: A police inspector had come forward with a very yo...\n",
      "作者风格概率: 1.00\n",
      "判断结果: 符合作者风格\n",
      "置信度: 1.00\n",
      "--------------------------------------------------\n",
      "文本: I confess that when first I made acquaintance with...\n",
      "作者风格概率: 0.00\n",
      "判断结果: 不符合作者风格\n",
      "置信度: 1.00\n",
      "--------------------------------------------------\n",
      "文本: Our team has decided to create the dataset ourselv...\n",
      "作者风格概率: 0.76\n",
      "判断结果: 符合作者风格\n",
      "置信度: 0.76\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 加载保存的模型和分词器\n",
    "model_path = \"../author_style_model\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def analyze_text_style(text):\n",
    "    \"\"\"分析文本是否符合作者的写作风格\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "    author_style_prob = probabilities[0][1].item()\n",
    "    \n",
    "    return {\n",
    "        \"author_style_probability\": author_style_prob,\n",
    "        \"is_author_style\": author_style_prob > 0.5,\n",
    "        \"confidence\": max(author_style_prob, 1-author_style_prob)\n",
    "    }\n",
    "\n",
    "# 测试示例\n",
    "test_texts = [\n",
    "    \"A police inspector had come forward with a very young medical student who was completing his forensic training at the municipal dispensary, and it was they who had ventilated the room and covered the body while waiting for Dr. Urbino to arrive. They greeted him with a solemnity that on this occasion had more of condolence than veneration, for no one was unaware of the degree of his friendship with Jeremiah de Saint-Amour. The eminent teacher shook hands with each of them, as he always did with every one of his pupils before beginning the daily class in general clinical medicine, and then, as if it were a flower, he grasped the hem of the blanket with the tips of his index finger and his thumb, and slowly uncovered the body with sacramental circumspection. Jeremiah de Saint-Amour was completely naked, stiff and twisted, eyes open, body blue, looking fifty years older than he had the night before. He had luminous pupils, yellowish beard and hair, and an old scar sewn with baling knots across his stomach. The use of crutches had made his torso and arms as broad as a galley slave’s, but his defenseless legs looked like an orphan’s. Dr. Juvenal Urbino studied him for a moment, his heart aching as it rarely had in the long years of his futile struggle against death.\",\n",
    "\n",
    "    \"I confess that when first I made acquaintance with Charles Strickland I never for a moment discerned that there was in him anything out of the ordinary. Yet now few will be found to deny his greatness. I do not speak of that greatness which is achieved by the fortunate politician or the successful soldier; that is a quality which belongs to the place he occupies rather than to the man; and a change of circumstances reduces it to very discreet proportions. The Prime Minister out of office is seen, too often, to have been but a pompous rhetorician, and the General without at! army is but the tame hero of a market town. The greatness of Charles Strickland was authentic. It may be that you do not like his art, but at all events you can hardly refuse it the tribute of your interest. He disturbs and arrests. The time has passed when he was an object of ridicule, and it is no longer a mark of eccentricity to defend or of perversity to extol him. His faults are accepted as the necessary complement to his merits. It is still possible to discuss his place in art, and the adulation of his admirers is perhaps no less capricious than the .disparagement of his detractors; but one thing cantilever be doubtful, and that is that he had genius. Tjo rqfy mind the most interesting thing in art is the personality of the artist ; and if that is singu- lar, I am willing to excuse a thousand faults. I suppose Velasquez was a better painter than El Greco, but custom stales one’s admiration for him: the Cretan, sensual and tragic, proffers the mystery of his soul like a standing sacrifice. The artist, painter, poet, or musician, by his decoration, sublime or beautiful, satisfies the aesthetic sense; but that is akin to the sexual instinct, anjl shares its barbarity: he lays before you also the greater gift of himself. To pursue his secret has something of the fascination of a detective story. It is a riddle which shares with the universe the merit of having no answer. The most insignificant of Strickland’s works suggests a personality which is strange, tormented, and complex; and it is this surely which prevents even those who do not like his pictures from being indifferent to them; it is this which has excited so curious an interest in his life and character. \",\n",
    "    \n",
    "    \"Our team has decided to create the dataset ourselves, consisting of articles from famous authors. The dataset structure includes work title, author, article content, and sentiment labels (positive, neutral, negative). We will select both Chinese and English articles in their original forms to avoid translation biases. Preprocessing will standardize the text by removing punctuation, converting text to lowercase, removing stopwords, and applying other necessary preprocessing techniques. Sentiment labels will be added through manual annotation and leveraging publicly available sentiment datasets. Our primary approach is to fine-tune BERT based on the collected dataset. Given our multilingual dataset, we have opted to use Multilingual BERT. If initial results are unsatisfactory, separate models may be trained for Chinese and English texts. Considering BERT's token limit (512 tokens), we plan to employ a sliding window segmentation approach. Should segmentation negatively impact stylistic judgment, we will explore alternative models like LSTM combined with BERT to maintain context continuity. Additionally, we will fine-tune BERT or RoBERTa for sentiment analysis, potentially incorporating multi-task learning to simultaneously optimize authorship and sentiment classification tasks.\",\n",
    "\n",
    "    \"In recent years, the rapid development of large language models (LLMs) has reshaped artificial intelligence research and applications. DeepSeek is an open-source AI research initiative dedicated to advancing natural language processing (NLP) through state-of-the-art LLMs. By offering open-access models and tools, DeepSeek aims to democratize AI capabilities and accelerate innovation in various fields. DeepSeek was established to address the growing need for transparency and accessibility in AI research. Unlike proprietary models from major tech companies, DeepSeek provides publicly available models that can be fine-tuned and deployed by researchers, developers, and businesses. This open approach fosters collaboration and ensures that AI advancements benefit a wider audience. DeepSeek's models are designed to support a wide range of NLP tasks, including text classification, sentiment analysis, question answering, and more. By leveraging large-scale pre-trained models, DeepSeek enables users to achieve high performance on various NLP benchmarks with minimal effort. DeepSeek's models are built on top of the Hugging Face Transformers library, a popular open-source framework for training and deploying transformer models. This integration allows users to easily access and utilize DeepSeek's models within their existing workflows. DeepSeek's models are available in multiple languages, making them suitable for global applications. Whether you're working on English, Chinese, Spanish, or other languages, DeepSeek provides models that can be fine-tuned for specific tasks and domains. DeepSeek's models are trained on diverse datasets to ensure robust performance across different languages and domains. By incorporating multilingual data and transfer learning techniques, DeepSeek's models can effectively handle various NLP tasks with high accuracy and efficiency. DeepSeek's models are continuously updated and refined to reflect the latest advancements in NLP research. By staying at the forefront of AI innovation, DeepSeek aims to empower users with cutting-edge tools and technologies for natural language processing. DeepSeek's mission is to democratize AI research and foster collaboration among researchers, developers, and businesses. By providing open-access models and resources, DeepSeek enables users to leverage state-of-the-art AI capabilities for a wide range of applications. Whether you're a student, researcher, developer, or business professional, DeepSeek offers tools and models that can accelerate your AI projects and drive innovation in the field of natural language processing.\",\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = analyze_text_style(text)\n",
    "    print(f\"文本: {text[:50]}...\")\n",
    "    print(f\"作者风格概率: {result['author_style_probability']:.2f}\")\n",
    "    print(f\"判断结果: {'符合作者风格' if result['is_author_style'] else '不符合作者风格'}\")\n",
    "    print(f\"置信度: {result['confidence']:.2f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
