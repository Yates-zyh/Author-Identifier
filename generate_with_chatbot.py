# -*- coding: utf-8 -*-
"""chatbot & generator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SS5buFSAo3yFXIODgVzGjkpUDksOgTx-
"""

import torch
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer, BertForSequenceClassification, BertTokenizer
import time
import os
import json
import shutil
from datetime import datetime, timedelta
from openai import OpenAI
import dotenv
from huggingface_hub import hf_hub_download, snapshot_download
import logging

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("author_style_api")

# 加载环境变量
dotenv.load_dotenv()

# 从环境变量中获取 API 密钥
api_key = os.environ.get("OPENAI_API_KEY")

# Author style model API
class AuthorStyleAPI:
    def __init__(self, token=None, use_remote=None):
        # 优先使用传入的token，否则从环境变量获取
        self.token = token or os.environ.get("GENERATION_TOKEN")
        
        # 确定是否使用远程模型（默认依赖于token是否存在）
        if use_remote is None:
            self.use_remote = self.token is not None
        else:
            self.use_remote = use_remote
            
        # 如果设置为远程但没有token，发出警告
        if self.use_remote and not self.token:
            logger.warning("Remote mode specified but no GENERATION_TOKEN found. Will attempt to use local model.")
            self.use_remote = False
            
        # 设置模型路径
        self.model_name = "fjxddy/author-stylegan"
        self.local_model_path = "author_style_model"  # 本地模型的根目录
        self.generators_path = os.path.join(self.local_model_path, "generators")  # 生成器模型路径
        self.discriminators_path = os.path.join(self.local_model_path, "discriminators")  # 判别器模型路径
        
        # 确保本地目录存在
        os.makedirs(self.generators_path, exist_ok=True)
        os.makedirs(self.discriminators_path, exist_ok=True)
        
        self.available_authors = [
            "Agatha_Christie",
            "Alexandre_Dumas",
            "Arthur_Conan_Doyle",
            "Charles_Dickens",
            "Charlotte_Brontë",
            "F._Scott_Fitzgerald",
            "García_Márquez",
            "Herman_Melville",
            "Jane_Austen",
            "Mark_Twain"
        ]
        self.loaded_models = {}  # 缓存已加载的模型
        self.last_request_time = None
        self.min_request_interval = 2  # 最小请求间隔（秒）
        
        logger.info(f"Initialized AuthorStyleAPI with {'remote' if self.use_remote else 'local'} mode")

    def _wait_for_rate_limit(self):
        """确保请求间隔符合速率限制"""
        if self.last_request_time is not None:
            elapsed = (datetime.now() - self.last_request_time).total_seconds()
            if elapsed < self.min_request_interval:
                time.sleep(self.min_request_interval - elapsed)
        self.last_request_time = datetime.now()

    def _download_model(self, author, model_type="generators", max_retries=3):
        """
        下载指定作者的模型到本地目录
        
        参数:
        - author: 作者名称
        - model_type: 模型类型，可选 "generators" 或 "discriminators"
        - max_retries: 最大重试次数
        
        返回:
        - success: 下载是否成功
        """
        if not self.token:
            logger.error("No token available for model download")
            return False
            
        remote_path = f"{model_type}/{author}"
        if model_type == "discriminators":
            remote_path = f"{remote_path}/best_model"
        
        local_path = os.path.join(self.local_model_path, remote_path)
        
        # 如果本地模型已存在，则跳过下载
        if os.path.exists(local_path) and os.path.exists(os.path.join(local_path, "config.json")):
            logger.info(f"Local model already exists at {local_path}")
            return True
            
        for attempt in range(max_retries):
            try:
                logger.info(f"Downloading {model_type} model for {author} (attempt {attempt+1}/{max_retries})...")
                self._wait_for_rate_limit()
                
                # 创建临时目录
                temp_dir = f"{local_path}_temp"
                os.makedirs(temp_dir, exist_ok=True)
                
                # 下载模型
                snapshot_download(
                    repo_id=self.model_name,
                    local_dir=temp_dir,
                    subfolder=remote_path,
                    token=self.token
                )
                
                # 移动文件到最终目录
                if os.path.exists(local_path):
                    shutil.rmtree(local_path)
                os.makedirs(os.path.dirname(local_path), exist_ok=True)
                shutil.move(temp_dir, local_path)
                
                logger.info(f"Model successfully downloaded to {local_path}")
                return True
            except Exception as e:
                if "429" in str(e):  # 速率限制错误
                    wait_time = (attempt + 1) * 10  # 等待时间指数增长
                    logger.warning(f"API rate limit reached. Waiting {wait_time} seconds...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"Error downloading model (attempt {attempt+1}/{max_retries}): {str(e)}")
                    # 清理临时目录
                    if os.path.exists(temp_dir):
                        shutil.rmtree(temp_dir)
                    
                    if attempt == max_retries - 1:
                        logger.error("Failed to download model after multiple retries")
                        return False
        return False
        
    def _load_model(self, author, max_retries=3):
        """加载指定作者的模型，带重试机制"""
        if author not in self.available_authors:
            raise ValueError(f"作者 {author} 不可用。请从以下作者中选择: {', '.join(self.available_authors)}")
            
        if author in self.loaded_models:
            return self.loaded_models[author]

        # 检查本地模型是否存在
        local_author_path = os.path.join(self.generators_path, author)
        local_model_exists = os.path.exists(local_author_path) and os.path.exists(os.path.join(local_author_path, "config.json"))
        
        # 优先使用本地模型，如果不存在且有token，则下载
        if local_model_exists:
            logger.info(f"Local generator model found for {author}, using local model")
            use_remote = False
        elif self.token:
            logger.info(f"Local generator model not found for {author}, will try to download")
            # 尝试下载生成器模型
            download_success = self._download_model(author, "generators")
            if download_success:
                logger.info(f"Successfully downloaded generator model for {author}, using local model")
                use_remote = False
            else:
                logger.warning(f"Failed to download generator model for {author}, will use remote model")
                use_remote = True
        else:
            logger.warning(f"Local generator model not found for {author} and no token available")
            use_remote = False  # 尝试使用本地路径（即使可能失败）

        for attempt in range(max_retries):
            try:
                if use_remote:
                    self._wait_for_rate_limit()
                    logger.info(f"Loading remote generator model for {author}... (attempt {attempt + 1}/{max_retries})")
                    tokenizer = AutoTokenizer.from_pretrained(
                        self.model_name,
                        subfolder=f"generators/{author}",
                        token=self.token
                    )
                    
                    model = AutoModelForCausalLM.from_pretrained(
                        self.model_name,
                        subfolder=f"generators/{author}",
                        token=self.token
                    )
                else:
                    logger.info(f"Loading local generator model for {author}... (attempt {attempt + 1}/{max_retries})")
                    local_author_path = os.path.join(self.generators_path, author)
                    if not os.path.exists(local_author_path) or not os.path.exists(os.path.join(local_author_path, "config.json")):
                        raise FileNotFoundError(f"Local model path not found or incomplete: {local_author_path}")
                        
                    tokenizer = AutoTokenizer.from_pretrained(local_author_path)
                    model = AutoModelForCausalLM.from_pretrained(local_author_path)
                
                # 确保tokenizer有pad_token
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                    
                self.loaded_models[author] = (tokenizer, model)
                return tokenizer, model
            except Exception as e:
                if "429" in str(e) and use_remote:  # 速率限制错误
                    wait_time = (attempt + 1) * 10  # 等待时间指数增长
                    logger.warning(f"Rate limit reached. Waiting {wait_time} seconds...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"Error loading model for {author}: {str(e)}")
                    if attempt == max_retries - 1:
                        # 如果是远程模式失败，尝试回退到另一种模式
                        if use_remote:
                            logger.warning("Remote model loading failed. Trying local model instead.")
                            use_remote = False
                            # 检查是否需要重新下载
                            if not os.path.exists(os.path.join(self.generators_path, author, "config.json")) and self.token:
                                self._download_model(author, "generators")
                            # 递归调用但设置新的模式
                            return self._load_model(author, max_retries)
                        elif self.token:  # 本地模式失败，尝试远程模式
                            logger.warning("Local model loading failed. Trying remote model instead.")
                            use_remote = True
                            return self._load_model(author, max_retries)
                        else:
                            raise  # 两种模式都失败，抛出异常

    def generate_text(self, author, num_samples=1, max_length=200, max_retries=3):
        """生成指定作者风格的文本样本，带重试机制"""
        if author not in self.available_authors:
            raise ValueError(f"作者 {author} 不可用。请从以下作者中选择: {', '.join(self.available_authors)}")

        for attempt in range(max_retries):
            try:
                tokenizer, model = self._load_model(author)
                samples = []

                for i in range(num_samples):
                    self._wait_for_rate_limit()
                    logger.info(f"Generating sample {i + 1}/{num_samples}...")
                    # 创建输入张量和注意力掩码
                    input_ids = torch.tensor([[tokenizer.bos_token_id]])
                    attention_mask = torch.ones_like(input_ids)

                    outputs = model.generate(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        max_length=max_length,
                        temperature=0.9,
                        top_k=40,
                        top_p=0.9,
                        repetition_penalty=1.2,
                        do_sample=True,
                        pad_token_id=tokenizer.pad_token_id
                    )
                    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                    samples.append(generated_text)

                return samples
            except Exception as e:
                if "429" in str(e) and self.use_remote:  # 速率限制错误
                    wait_time = (attempt + 1) * 10  # 等待时间指数增长
                    logger.warning(f"Rate limit reached. Waiting {wait_time} seconds...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"Error generating text: {str(e)}")
                    if attempt == max_retries - 1:
                        raise  # 所有重试都失败，抛出异常

    def evaluate_text(self, text, author, max_retries=3):
        """评估文本与指定作者风格的匹配度，带重试机制"""
        if author not in self.available_authors:
            raise ValueError(f"作者 {author} 不可用。请从以下作者中选择: {', '.join(self.available_authors)}")

        # 检查本地判别器模型是否存在
        local_disc_path = os.path.join(self.discriminators_path, author, "best_model")
        local_model_exists = os.path.exists(local_disc_path) and os.path.exists(os.path.join(local_disc_path, "config.json"))
        
        # 优先使用本地模型，如果不存在且有token，则下载
        if local_model_exists:
            logger.info(f"Local discriminator model found for {author}, using local model")
            use_remote = False
        elif self.token:
            logger.info(f"Local discriminator model not found for {author}, will try to download")
            # 尝试下载判别器模型
            download_success = self._download_model(author, "discriminators")
            if download_success:
                logger.info(f"Successfully downloaded discriminator model for {author}, using local model")
                use_remote = False
            else:
                logger.warning(f"Failed to download discriminator model for {author}, will use remote model")
                use_remote = True
        else:
            logger.warning(f"Local discriminator model not found for {author} and no token available")
            use_remote = False  # 尝试使用本地路径（即使可能失败）

        for attempt in range(max_retries):
            try:
                self._wait_for_rate_limit()
                
                if use_remote:
                    # 远程模式：从Hugging Face加载
                    logger.info(f"Loading remote discriminator model for {author}... (attempt {attempt + 1}/{max_retries})")
                    tokenizer = BertTokenizer.from_pretrained(
                        self.model_name,
                        subfolder=f"discriminators/{author}/best_model",
                        token=self.token
                    )
                    model = BertForSequenceClassification.from_pretrained(
                        self.model_name,
                        subfolder=f"discriminators/{author}/best_model",
                        token=self.token
                    )

                    # 加载作者标签
                    label_path = f"discriminators/{author}/best_model/label_names.json"
                    try:
                        label_file = hf_hub_download(
                            repo_id=self.model_name,
                            filename=label_path,
                            token=self.token
                        )
                        with open(label_file, "r") as f:
                            author_labels = json.load(f)
                    except Exception as e:
                        logger.warning(f"Could not load remote label names: {str(e)}")
                        author_labels = [None, author]  # 默认标签
                else:
                    # 本地模式：从本地路径加载
                    logger.info(f"Loading local discriminator model for {author}... (attempt {attempt + 1}/{max_retries})")
                    if not os.path.exists(local_disc_path) or not os.path.exists(os.path.join(local_disc_path, "config.json")):
                        raise FileNotFoundError(f"Local discriminator path not found or incomplete: {local_disc_path}")
                        
                    tokenizer = BertTokenizer.from_pretrained(local_disc_path)
                    model = BertForSequenceClassification.from_pretrained(local_disc_path)
                    
                    # 加载本地标签
                    local_label_path = os.path.join(local_disc_path, "label_names.json")
                    try:
                        with open(local_label_path, "r") as f:
                            author_labels = json.load(f)
                    except Exception as e:
                        logger.warning(f"Could not load local label names: {str(e)}")
                        author_labels = [None, author]  # 默认标签

                # 创建作者索引映射
                author_indices = {author: idx for idx, author in enumerate(author_labels) if author is not None}
                author_idx = author_indices.get(author, 1)  # 默认索引为1

                # 编码文本
                inputs = tokenizer(
                    text,
                    return_tensors="pt",
                    padding='max_length',
                    truncation=True,
                    max_length=512
                )

                # 预测
                with torch.no_grad():
                    outputs = model(**inputs)
                    logits = outputs.logits
                    probs = F.softmax(logits, dim=1)
                    score = probs[0][author_idx].item()

                return score
            except Exception as e:
                if "429" in str(e) and use_remote:  # 速率限制错误
                    wait_time = (attempt + 1) * 10  # 等待时间指数增长
                    logger.warning(f"Rate limit reached. Waiting {wait_time} seconds...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"Error evaluating text: {str(e)}")
                    if attempt == max_retries - 1:
                        # 如果是远程模式失败，尝试回退到另一种模式
                        if use_remote and not self.use_remote:
                            logger.warning("Remote evaluation failed. Trying local model instead.")
                            use_remote = False
                            # 检查是否需要重新下载
                            if not os.path.exists(os.path.join(local_disc_path, "config.json")) and self.token:
                                self._download_model(author, "discriminators")
                            # 递归调用但设置新的模式
                            return self.evaluate_text(text, author)
                        elif not use_remote and self.token:  # 本地模式失败，尝试远程模式
                            logger.warning("Local evaluation failed. Trying remote model instead.")
                            use_remote = True
                            return self.evaluate_text(text, author)
                        else:
                            raise  # 两种模式都失败，抛出异常

    def generate_best_sample(self, author, num_samples=10, max_length=200):
        """生成多个样本并返回评分最高的一个"""
        samples = self.generate_text(author, num_samples, max_length)
        best_sample = None
        best_score = -1

        for i, sample in enumerate(samples):
            try:
                logger.info(f"Evaluating sample {i + 1}/{len(samples)}...")
                score = self.evaluate_text(sample, author)
                if score > best_score:
                    best_score = score
                    best_sample = sample
            except Exception as e:
                logger.error(f"Error evaluating sample: {str(e)}")
                continue

        return best_sample, best_score

# DeepSeek API客户端创建函数
def create_deepseek_client(api_key=None):
    """
    创建DeepSeek API客户端
    
    参数:
    - api_key: API密钥，如不提供则尝试从环境变量获取
    
    返回:
    - client: OpenAI客户端对象
    """
    # 如果没有提供API密钥，尝试从环境变量获取
    if not api_key:
        api_key = os.environ.get("OPENAI_API_KEY")
        
    if not api_key:
        raise ValueError("No DeepSeek API key provided and none found in environment variables")
        
    # 创建并返回客户端
    return OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

# DeepSeek conversation function
def chat_with_deepseek(author, api_key=None):
    """
    使用DeepSeek API生成指定作者风格的文本
    
    参数:
    - author: 作者名称
    - api_key: 可选的API密钥，如不提供则尝试从环境变量获取
    
    返回:
    - text: 生成的文本
    """
    try:
        # 创建DeepSeek客户端
        client = create_deepseek_client(api_key)
        
        # 设置初始对话消息
        conversation = [
            {"role": "system", "content": f"You are a helpful assistant that mimics the style of {author}."}
        ]

        # 请求文本生成
        user_input = "Please generate a text sample."
        conversation.append({"role": "user", "content": user_input})

        # 调用DeepSeek API获取响应
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=conversation,
            stream=False
        )

        # 获取DeepSeek回复
        deepseek_reply = response.choices[0].message.content
        logger.info(f"DeepSeek generated text for {author}")

        return deepseek_reply
    except Exception as e:
        logger.error(f"Error when calling DeepSeek API: {str(e)}")
        raise

# Main program
def main():
    # 从环境变量获取 Hugging Face token
    token = os.environ.get("GENERATION_TOKEN")
    api = AuthorStyleAPI(token)

    print("Available authors:")
    for author in api.available_authors:
        print(f"- {author}")

    while True:
        author = input("\nEnter author name (or 'quit' to exit): ")
        if author.lower() == 'quit':
            break

        if author not in api.available_authors:
            print(f"Author {author} not available. Please choose from the list above.")
            continue

        try:
            # Generate text using own model
            print(f"\nGenerating text in the style of {author} using your own model...")
            best_sample, score = api.generate_best_sample(author)
            print("\nBest generated text from your model:")
            print(best_sample)
            print(f"Style match score: {score:.4f}")

            # Generate text using DeepSeek API
            print("\nDo you want to generate text using DeepSeek API? (y/n)")
            deepseek_choice = input("> ")
            
            if deepseek_choice.lower() == 'y':
                # 检查环境变量中是否有API密钥
                api_key = os.environ.get("OPENAI_API_KEY")
                
                if not api_key:
                    print("\nNo DeepSeek API key found in environment variables.")
                    print("Do you want to enter an API key? (y/n)")
                    key_choice = input("> ")
                    
                    if key_choice.lower() == 'y':
                        api_key = input("Enter your DeepSeek API key: ")
                    else:
                        print("Skipping DeepSeek text generation.")
                        continue
                
                print(f"\nGenerating text in the style of {author} using DeepSeek...")
                deepseek_text = chat_with_deepseek(author, api_key)
                print("\nDeepSeek generated text:")
                print(deepseek_text)

                # Evaluate DeepSeek text style matching score
                deepseek_score = api.evaluate_text(deepseek_text, author)
                print(f"\nDeepSeek generated text style match score: {deepseek_score:.4f}")

                # Compare the scores and print the result
                if score > deepseek_score:
                    print("\nYour model generated better text based on style match score.")
                elif deepseek_score > score:
                    print("\nDeepSeek generated better text based on style match score.")
                else:
                    print("\nBoth models generated text with the same style match score.")

        except Exception as e:
            print(f"Error: {str(e)}")

if __name__ == "__main__":
    main()